import pandas as pd
import numpy as np
from sklearn.ensemble import (
    RandomForestRegressor, ExtraTreesRegressor, StackingRegressor, VotingRegressor, GradientBoostingRegressor
)
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.impute import KNNImputer
from sklearn.metrics import mean_squared_error, r2_score
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st
import os

# Hugging Face Token
hf_token = os.getenv("HF_TOKEN")
if not hf_token:
    st.error("Hugging Face í† í°ì´ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# ë°ì´í„° ë¡œë“œ
try:
    train_data = pd.read_csv("data/final_output_train.csv")
    test_data = pd.read_csv("data/final_output_test.csv")
except FileNotFoundError:
    st.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# íƒ€ê²Ÿ ë° í”¼ì²˜ ë¶„ë¦¬
target_columns = ['DURATION_MIN', 'MBLZ_FFPWR_CNT', 'LYCRG_FIREMAN_CNT']
X_train = train_data.drop(columns=target_columns)
y_train = train_data[target_columns]
X_test = test_data.drop(columns=target_columns)
y_test = test_data[target_columns]

# Imputer ì •ì˜
knn_imputer = KNNImputer(n_neighbors=5)
X_train_imputed = knn_imputer.fit_transform(X_train)
X_test_imputed = knn_imputer.transform(X_test)

# ëª¨ë¸ ì •ì˜
final_models = {
    'DURATION_MIN': StackingRegressor(estimators=[
        ('ridge', Ridge()),
        ('extra_trees', ExtraTreesRegressor(
            n_estimators=88, 
            max_depth=17, 
            random_state=42
        )),
        ('svr', SVR()),
        ('decision_tree', DecisionTreeRegressor(
            max_depth=17, 
            random_state=42
        )),
        ('random_forest', RandomForestRegressor(
            n_estimators=88, 
            max_depth=17, 
            min_samples_split=9, 
            random_state=42
        ))
    ], final_estimator=Ridge()),

    'MBLZ_FFPWR_CNT': VotingRegressor(estimators=[
        ('ridge', Ridge(alpha=0.003253527491118228)),
        ('random_forest', RandomForestRegressor(
            n_estimators=300, 
            max_depth=7, 
            min_samples_split=9, 
            min_samples_leaf=4, 
            random_state=62
        ))
    ]),

    'LYCRG_FIREMAN_CNT': VotingRegressor(estimators=[
        ('ridge', Ridge()),
        ('extra_trees', ExtraTreesRegressor(
            n_estimators=446, 
            max_depth=8, 
            random_state=54
        )),
        ('gradient_boosting', GradientBoostingRegressor(
            n_estimators=446, 
            max_depth=8, 
            learning_rate=0.1693, 
            random_state=54
        )),
        ('random_forest', RandomForestRegressor(
            n_estimators=446, 
            max_depth=8, 
            random_state=54
        ))
    ])
}

# ëª¨ë¸ í•™ìŠµ
for target, model in final_models.items():
    model.fit(X_train_imputed, y_train[target])

# LLaMA3 ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B", token=hf_token)
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B", token=hf_token)

# íŒ¨ë”© í† í° ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Streamlit App
st.title("ì‚°ë¶ˆ í”¼í•´ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜")
st.sidebar.header("ì…ë ¥ê°’ ì„¤ì •")

# ì‚¬ìš©ì ì…ë ¥
disabled_region = st.sidebar.text_input("ìœ„ì¹˜", "ê°•ì›ë„", disabled=True)  # ì…ë ¥ ë¹„í™œì„±í™”
damage_area = st.sidebar.number_input("ì´ˆê¸° í”¼í•´ ê·œëª¨ (í—¥íƒ€ë¥´)", min_value=1, value=100)
fireman_count = st.sidebar.number_input("ì†Œë°© ì¸ë ¥ ìˆ˜", min_value=1, value=50)

if st.sidebar.button("ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"):
    # ì…ë ¥ê°’ ì²˜ë¦¬
    X_input = pd.DataFrame(
        np.full((1, X_train.shape[1]), np.nan),
        columns=X_train.columns
    )
    X_input.iloc[0, 0] = damage_area
    X_input.iloc[0, 1] = fireman_count

    # KNN Imputerë¡œ ë‚˜ë¨¸ì§€ ê°’ì„ ì±„ì›€
    X_input_imputed = knn_imputer.transform(X_input)

    # ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred_duration = final_models['DURATION_MIN'].predict(X_input_imputed)[0]
    y_pred_mblz = final_models['MBLZ_FFPWR_CNT'].predict(X_input_imputed)[0]
    y_pred_lycrg = final_models['LYCRG_FIREMAN_CNT'].predict(X_input_imputed)[0]

    # LLaMA3 ìš”ì•½ ìƒì„±
    prompt = f"""
    ì‚°ë¶ˆ í”¼í•´ ì˜ˆì¸¡ ìš”ì•½:
    - ìœ„ì¹˜: ê°•ì›ë„
    - ì˜ˆìƒ í”¼í•´ ê·œëª¨ëŠ” {damage_area}í—¥íƒ€ë¥´ë¡œ ìƒë‹¹í•œ í¬ê¸°ì…ë‹ˆë‹¤.
    - ì†Œë°© ì¸ë ¥ {fireman_count}ëª…ì´ íˆ¬ì…ë  ì˜ˆì •ì´ë©°, ì˜ˆìƒ ì§„í™” ì‹œê°„ì€ ì•½ {y_pred_duration:.2f}ë¶„ì…ë‹ˆë‹¤.
    - ì‹ ì†í•œ ëŒ€ì‘ì´ í•„ìš”í•˜ë©° ì¶”ê°€ ì¥ë¹„ì™€ ìì›ì´ ìš”êµ¬ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
    """
    input_ids = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).input_ids
    output = model.generate(input_ids, max_length=512, temperature=0.7)
    llama_summary = tokenizer.decode(output[0], skip_special_tokens=True)

    # ê²°ê³¼ ì¶œë ¥
    st.header("ì˜ˆì¸¡ ê²°ê³¼")
    st.write(f"ğŸ“ **ìœ„ì¹˜**: ê°•ì›ë„")
    st.write(f"ğŸ”¥ **ì´ˆê¸° í”¼í•´ ê·œëª¨**: {damage_area} í—¥íƒ€ë¥´")
    st.write(f"ğŸš’ **ì†Œë°© ì¸ë ¥**: {fireman_count}ëª…")
    st.write(f"â³ **ì˜ˆìƒ ì§„í™” ì‹œê°„**: {y_pred_duration:.2f} ë¶„")
    st.write(f"ğŸ’§ **ì˜ˆìƒ ì†Œë°© ì„¤ë¹„ ì‚¬ìš©ëŸ‰**: {y_pred_mblz:.2f} ëŒ€")
    st.write(f"ğŸ‘©â€ğŸš’ **ì˜ˆìƒ ì†Œë°©ê´€ ìˆ˜**: {y_pred_lycrg:.2f} ëª…")

    st.header("ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½")
    st.write(llama_summary)
